# for gas
#ifdef __linux__
  #define PRE(x) x
  #define TYPE(x) .type x, @function
  #define SIZE(x) .size x, .-x
#else
  #ifdef _WIN32
    #define PRE(x) x
  #else
    #define PRE(x) _ ## x
  #endif
  #define TYPE(x)
  #define SIZE(x)
#endif
.data
PRE(p):
.quad 13402431016077863595, 2210141511517208575, 7435674573564081700, 7239337960414712511, 5412103778470702295, 1873798617647539866
PRE(zero):
.quad 0, 0, 0, 0, 0, 0
PRE(ip):
.quad 9940570264628428797
.text
.align 16
.global PRE(mcl_fp_add)
PRE(mcl_fp_add):
TYPE(mcl_fp_add)
push %rbx
push %rbp
push %r12
push %r13
push %r14
mov (%rsi), %rcx
add (%rdx), %rcx
mov 8(%rsi), %r8
adc 8(%rdx), %r8
mov 16(%rsi), %r9
adc 16(%rdx), %r9
mov 24(%rsi), %r10
adc 24(%rdx), %r10
mov 32(%rsi), %r11
adc 32(%rdx), %r11
mov 40(%rsi), %rbx
adc 40(%rdx), %rbx
mov %rcx, %rbp
mov $13402431016077863595, %rax
sub %rax, %rbp
mov %r8, %r12
mov $2210141511517208575, %rax
sbb %rax, %r12
mov %r9, %r13
mov $7435674573564081700, %rax
sbb %rax, %r13
mov %r10, %r14
mov $7239337960414712511, %rax
sbb %rax, %r14
mov %r11, %rsi
mov $5412103778470702295, %rax
sbb %rax, %rsi
mov %rbx, %rdx
mov $1873798617647539866, %rax
sbb %rax, %rdx
cmovc %rcx, %rbp
mov %rbp, (%rdi)
cmovc %r8, %r12
mov %r12, 8(%rdi)
cmovc %r9, %r13
mov %r13, 16(%rdi)
cmovc %r10, %r14
mov %r14, 24(%rdi)
cmovc %r11, %rsi
mov %rsi, 32(%rdi)
cmovc %rbx, %rdx
mov %rdx, 40(%rdi)
pop %r14
pop %r13
pop %r12
pop %rbp
pop %rbx
ret
SIZE(mcl_fp_add)
.align 16
.global PRE(mcl_fp_sub)
PRE(mcl_fp_sub):
TYPE(mcl_fp_sub)
push %rbx
push %rbp
push %r12
push %r13
push %r14
mov (%rsi), %rcx
sub (%rdx), %rcx
mov 8(%rsi), %r8
sbb 8(%rdx), %r8
mov 16(%rsi), %r9
sbb 16(%rdx), %r9
mov 24(%rsi), %r10
sbb 24(%rdx), %r10
mov 32(%rsi), %r11
sbb 32(%rdx), %r11
mov 40(%rsi), %rbx
sbb 40(%rdx), %rbx
sbb %rax, %rax
mov $13402431016077863595, %rbp
and %rax, %rbp
mov $2210141511517208575, %r12
and %rax, %r12
mov $7435674573564081700, %r13
and %rax, %r13
mov $7239337960414712511, %r14
and %rax, %r14
mov $5412103778470702295, %rsi
and %rax, %rsi
mov $1873798617647539866, %rdx
and %rax, %rdx
add %rbp, %rcx
mov %rcx, (%rdi)
adc %r12, %r8
mov %r8, 8(%rdi)
adc %r13, %r9
mov %r9, 16(%rdi)
adc %r14, %r10
mov %r10, 24(%rdi)
adc %rsi, %r11
mov %r11, 32(%rdi)
adc %rdx, %rbx
mov %rbx, 40(%rdi)
pop %r14
pop %r13
pop %r12
pop %rbp
pop %rbx
ret
SIZE(mcl_fp_sub)
.align 16
.global PRE(mcl_fp_mul)
PRE(mcl_fp_mul):
TYPE(mcl_fp_mul)
push %rbx
push %rbp
push %r12
push %r13
push %r14
mov %rdx, %r11
lea PRE(p)(%rip), %rax
mov (%r11), %rdx
mulx (%rsi), %rcx, %r8
mulx 8(%rsi), %r13, %r9
add %r13, %r8
mulx 16(%rsi), %r13, %r10
adc %r13, %r9
mulx 24(%rsi), %r13, %rbx
adc %r13, %r10
mulx 32(%rsi), %r13, %rbp
adc %r13, %rbx
mulx 40(%rsi), %r13, %r12
adc %r13, %rbp
adc $0, %r12
mov $9940570264628428797, %rdx
imul %rcx, %rdx
xor %r13, %r13
mulx (%rax), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 8(%rax), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 16(%rax), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 24(%rax), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 32(%rax), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 40(%rax), %r14, %r13
adox %r14, %rbp
adox %r13, %r12
adc $0, %r12
mov 8(%r11), %rdx
xor %rcx, %rcx
mulx (%rsi), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 8(%rsi), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 16(%rsi), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 24(%rsi), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 32(%rsi), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 40(%rsi), %r14, %r13
adox %r14, %r12
adox %r13, %rcx
adc $0, %rcx
mov $9940570264628428797, %rdx
imul %r8, %rdx
xor %r13, %r13
mulx (%rax), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 8(%rax), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 16(%rax), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 24(%rax), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 32(%rax), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 40(%rax), %r14, %r13
adox %r14, %r12
adox %r13, %rcx
adc $0, %rcx
mov 16(%r11), %rdx
xor %r8, %r8
mulx (%rsi), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 8(%rsi), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 16(%rsi), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 24(%rsi), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 32(%rsi), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 40(%rsi), %r14, %r13
adox %r14, %rcx
adox %r13, %r8
adc $0, %r8
mov $9940570264628428797, %rdx
imul %r9, %rdx
xor %r13, %r13
mulx (%rax), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 8(%rax), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 16(%rax), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 24(%rax), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 32(%rax), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 40(%rax), %r14, %r13
adox %r14, %rcx
adox %r13, %r8
adc $0, %r8
mov 24(%r11), %rdx
xor %r9, %r9
mulx (%rsi), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 8(%rsi), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 16(%rsi), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 24(%rsi), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 32(%rsi), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 40(%rsi), %r14, %r13
adox %r14, %r8
adox %r13, %r9
adc $0, %r9
mov $9940570264628428797, %rdx
imul %r10, %rdx
xor %r13, %r13
mulx (%rax), %r14, %r13
adox %r14, %r10
adcx %r13, %rbx
mulx 8(%rax), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 16(%rax), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 24(%rax), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 32(%rax), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 40(%rax), %r14, %r13
adox %r14, %r8
adox %r13, %r9
adc $0, %r9
mov 32(%r11), %rdx
xor %r10, %r10
mulx (%rsi), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 8(%rsi), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 16(%rsi), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 24(%rsi), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 32(%rsi), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 40(%rsi), %r14, %r13
adox %r14, %r9
adox %r13, %r10
adc $0, %r10
mov $9940570264628428797, %rdx
imul %rbx, %rdx
xor %r13, %r13
mulx (%rax), %r14, %r13
adox %r14, %rbx
adcx %r13, %rbp
mulx 8(%rax), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 16(%rax), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 24(%rax), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 32(%rax), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 40(%rax), %r14, %r13
adox %r14, %r9
adox %r13, %r10
adc $0, %r10
mov 40(%r11), %rdx
xor %rbx, %rbx
mulx (%rsi), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 8(%rsi), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 16(%rsi), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 24(%rsi), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 32(%rsi), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 40(%rsi), %r14, %r13
adox %r14, %r10
adox %r13, %rbx
adc $0, %rbx
mov $9940570264628428797, %rdx
imul %rbp, %rdx
xor %r13, %r13
mulx (%rax), %r14, %r13
adox %r14, %rbp
adcx %r13, %r12
mulx 8(%rax), %r14, %r13
adox %r14, %r12
adcx %r13, %rcx
mulx 16(%rax), %r14, %r13
adox %r14, %rcx
adcx %r13, %r8
mulx 24(%rax), %r14, %r13
adox %r14, %r8
adcx %r13, %r9
mulx 32(%rax), %r14, %r13
adox %r14, %r9
adcx %r13, %r10
mulx 40(%rax), %r14, %r13
adox %r14, %r10
adox %r13, %rbx
adc $0, %rbx
mov %r12, %rbp
mov %rcx, %rsi
mov %r8, %r11
mov %r9, %rdx
mov %r10, %r13
mov %rbx, %r14
sub (%rax), %r12
sbb 8(%rax), %rcx
sbb 16(%rax), %r8
sbb 24(%rax), %r9
sbb 32(%rax), %r10
sbb 40(%rax), %rbx
cmovc %rbp, %r12
cmovc %rsi, %rcx
cmovc %r11, %r8
cmovc %rdx, %r9
cmovc %r13, %r10
cmovc %r14, %rbx
mov %r12, (%rdi)
mov %rcx, 8(%rdi)
mov %r8, 16(%rdi)
mov %r9, 24(%rdi)
mov %r10, 32(%rdi)
mov %rbx, 40(%rdi)
pop %r14
pop %r13
pop %r12
pop %rbp
pop %rbx
ret
SIZE(mcl_fp_mul)
